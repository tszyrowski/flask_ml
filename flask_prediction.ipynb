{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 17:46:05.200924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-05 17:46:05.297072: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-05 17:46:05.300642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-05 17:46:05.300652: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-05 17:46:05.319098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-05 17:46:05.790172: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 17:46:05.790223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 17:46:05.790241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_used = \"mog_model_1_4.743589398682443.h5\"\n",
    "\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(r\"auto-mpg.csv\", na_values=[\"NA\", \"?\"])\n",
    "cols = [x for x in df.columns if x not in (\"mpg\", \"car name\")]\n",
    "expected = {}\n",
    "for i, name in enumerate(cols):\n",
    "    expected[name] = {}\n",
    "    expected[name][\"min\"] = df[name].min() \n",
    "    expected[name][\"max\"] = df[name].max()\n",
    "# for key, _ in expected.items():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 25)                200       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 471\n",
      "Trainable params: 471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 17:46:06.330565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-05 17:46:06.330583: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-05 17:46:06.330596: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NumericalCatalyst): /proc/driver/nvidia/version does not exist\n",
      "2022-11-05 17:46:06.330769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(os.path.join(os.getcwd(), model_used))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/api\", methods=[\"POST\"])\n",
    "def mpg_prediction():\n",
    "    content = request.json\n",
    "    errors = []\n",
    "    for name in content:\n",
    "        if name in expected:\n",
    "            expected_min = expected[name][\"min\"]\n",
    "            expected_max = expected[name][\"max\"]\n",
    "            value = content[name]\n",
    "            if value < expected_min or value > expected_max:\n",
    "                errors.append(f\"{name} out of bounds: {expected_min} and {expected_max}\")\n",
    "        else:\n",
    "            errors.append(f\"Unexpected field: {name}\")\n",
    "    for name in expected:\n",
    "        if name not in content:\n",
    "            errors.append(f\"Missing value: {name}\")\n",
    "    if len(errors) < 1:\n",
    "        x = np.zeros((1, 7))\n",
    "        x[0,0] = content[\"cylinders\"]\n",
    "        x[0,1] = content[\"displacement\"]\n",
    "        x[0,2] = content[\"horsepower\"]\n",
    "        x[0,3] = content[\"weight\"]\n",
    "        x[0,4] = content[\"acceleration\"]\n",
    "        x[0,5] = content[\"model year\"]\n",
    "        x[0,6] = content[\"origin\"]\n",
    "\n",
    "        prediction = model.predict(x)\n",
    "        mpg = float(prediction[0])\n",
    "        response = {\"id\": str(uuid.uuid4()), \"mpg\": mpg, \"errors\": errors}\n",
    "    else:\n",
    "        response = {\"id\": str(uuid.uuid4()), \"errors\": errors}\n",
    "    return jsonify(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Nov/2022 17:46:09] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Nov/2022 17:46:28] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Nov/2022 17:47:47] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Nov/2022 17:47:55] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Nov/2022 17:48:04] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Nov/2022 17:48:27] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "92da74382ff4b4911b8ec29eb996982fa4e7766dedcc102440e2956b7f725717"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
